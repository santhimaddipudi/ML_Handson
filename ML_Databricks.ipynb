{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLXceB4oWoEAh2+5OrMrHZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhimaddipudi/ML_Handson/blob/main/ML_Databricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UVhMjzbzlVK",
        "outputId": "f8841c07-8abc-41ca-fa50-81814f4d9666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ca0dea7e0d034efb16aaa6f5d6b948317a2ef566efc6db7ec74667e197fbf279\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ImputerExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "NY-tlGso1EI7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1.0, float(\"nan\")),\n",
        "    (2.0, float(\"nan\")),\n",
        "    (float(\"nan\"), 3.0),\n",
        "    (4.0, 4.0),\n",
        "    (5.0, 5.0)\n",
        "], [\"a\", \"b\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifb3PBf_z1sK",
        "outputId": "1b6e8f43-ddd6-4cae-f77b-c303f0a3b333"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|  a|  b|\n",
            "+---+---+\n",
            "|1.0|NaN|\n",
            "|2.0|NaN|\n",
            "|NaN|3.0|\n",
            "|4.0|4.0|\n",
            "|5.0|5.0|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"a\", \"b\"])\n",
        "model = imputer.fit(df)\n",
        "\n",
        "model.transform(df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9oyom2t1Mj3",
        "outputId": "6a49edd3-6ebe-477b-bd3b-b2f8f1ef4998"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|  a|  b|\n",
            "+---+---+\n",
            "|1.0|4.0|\n",
            "|2.0|4.0|\n",
            "|3.0|3.0|\n",
            "|4.0|4.0|\n",
            "|5.0|5.0|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extractors"
      ],
      "metadata": {
        "id": "8dNh5bP9lAGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.tf-idf**:Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by t\n",
        ", a document by d\n",
        ", and the corpus by D\n",
        ". Term frequency TF(t,d)\n",
        " is the number of times that term t\n",
        " appears in document d\n",
        ", while document frequency DF(t,D)\n",
        " is the number of documents that contains term t\n",
        "."
      ],
      "metadata": {
        "id": "xi0hGgu7lRnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show()\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "featurizedData.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4igO8K9k_hh",
        "outputId": "fc1b9a61-e271-4064-e30e-709d76851558"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
            "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
            "|  1.0|Logistic regressi...|[logistic, regres...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|         rawFeatures|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(10,[3,6,8],[1.0,...|\n",
            "|  0.0|I wish Java could...|[i, wish, java, c...|(10,[0,2,3,5,6,7]...|\n",
            "|  1.0|Logistic regressi...|[logistic, regres...|(10,[1,3,4,6,9],[...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "rescaledData.show()\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1W8Og-QnQ-s",
        "outputId": "0b175216-508a-4dfd-c61a-2e22fe254847"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(10,[3,6,8],[1.0,...|(10,[3,6,8],[0.0,...|\n",
            "|  0.0|I wish Java could...|[i, wish, java, c...|(10,[0,2,3,5,6,7]...|(10,[0,2,3,5,6,7]...|\n",
            "|  1.0|Logistic regressi...|[logistic, regres...|(10,[1,3,4,6,9],[...|(10,[1,3,4,6,9],[...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(10,[3,6,8],[0.0,...|\n",
            "|  0.0|(10,[0,2,3,5,6,7]...|\n",
            "|  1.0|(10,[1,3,4,6,9],[...|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2**)Word2Vec:**\n",
        "Word2Vec is an Estimator which takes sequences of words representing documents and trains a Word2VecModel. The model maps each word to a unique fixed-size vector. The Word2VecModel transforms each document into a vector using the average of all words in the document;"
      ],
      "metadata": {
        "id": "tgATw6arpPNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
        "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
        "    (\"Logistic regression models are neat\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI254nfGpT_l",
        "outputId": "1b7da1fa-6693-41ad-b98b-3a1a77c27fca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: [Hi, I, heard, about, Spark] => \n",
            "Vector: [-0.06992989811114968,-0.01609008610248566,0.0014188162982463838]\n",
            "\n",
            "Text: [I, wish, Java, could, use, case, classes] => \n",
            "Vector: [-0.008882724546960421,-0.01684619652639542,0.025261282920837402]\n",
            "\n",
            "Text: [Logistic, regression, models, are, neat] => \n",
            "Vector: [-0.011323881149291993,0.015113681554794312,0.09331511408090593]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3)CountVectorizer**\n",
        "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel."
      ],
      "metadata": {
        "id": "xZcCLTqupd7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with a ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahI4BstupdrE",
        "outputId": "1eae4136-6d1d-4be0-cf31-a812aa8bf2e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+-------------------------+\n",
            "|id |words          |features                 |\n",
            "+---+---------------+-------------------------+\n",
            "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
            "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
            "+---+---------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4)FeatureHasher**\n",
        "Feature hashing projects a set of categorical or numerical features into a feature vector of specified dimension (typically substantially smaller than that of the original feature space). This is done using the hashing trick to map features to indices in the feature vector."
      ],
      "metadata": {
        "id": "_CY2nJZ4po2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import FeatureHasher\n",
        "\n",
        "dataset = spark.createDataFrame([\n",
        "    (2.2, True, \"1\", \"foo\"),\n",
        "    (3.3, False, \"2\", \"bar\"),\n",
        "    (4.4, False, \"3\", \"baz\"),\n",
        "    (5.5, False, \"4\", \"foo\")\n",
        "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
        "\n",
        "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
        "                       outputCol=\"features\")\n",
        "\n",
        "featurized = hasher.transform(dataset)\n",
        "featurized.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzrgEbc0pooG",
        "outputId": "4d5059c4-9e7f-4d04-b023-54662b8728a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+---------+------+--------------------------------------------------------+\n",
            "|real|bool |stringNum|string|features                                                |\n",
            "+----+-----+---------+------+--------------------------------------------------------+\n",
            "|2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
            "|3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
            "|4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
            "|5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n",
            "+----+-----+---------+------+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Transformers"
      ],
      "metadata": {
        "id": "yy_479hMp2Fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Tokenizer"
      ],
      "metadata": {
        "id": "aF1i9_bpp8tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi I heard about Spark\"),\n",
        "    (1, \"I wish Java could use case classes\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
        "#with RegexTokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
        "regexTokenized.select(\"sentence\", \"words\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwk6f3ekp_iS",
        "outputId": "e35232ca-0ff4-4202-e92d-b3af530f71e0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.stop words remover"
      ],
      "metadata": {
        "id": "o0bHXI_xrA4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stop words remover\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC0tX_pirC9A",
        "outputId": "2eb2dc9c-6d20-4e40-c981-70bf8aca8ad0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------+--------------------+\n",
            "|id |raw                         |filtered            |\n",
            "+---+----------------------------+--------------------+\n",
            "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
            "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
            "+---+----------------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.n-gram**\n",
        "An n-gram is a sequence of n\n",
        " tokens (typically words) for some integer n\n",
        ". The NGram class can be used to transform input features into n\n",
        "-grams."
      ],
      "metadata": {
        "id": "-2w_sSBNrQvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "\n",
        "wordDataFrame = spark.createDataFrame([\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRd9ceVhrTKH",
        "outputId": "2ee896fa-53fe-4e35-9d31-5eb9336dadbf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Binarizer :\n",
        "Binarization is the process of thresholding numerical features to binary (0/1) features."
      ],
      "metadata": {
        "id": "5VkZKsPQrdWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "\n",
        "continuousDataFrame = spark.createDataFrame([\n",
        "    (0, 0.1),\n",
        "    (1, 0.8),\n",
        "    (2, 0.5),\n",
        "    (3, 0.6)\n",
        "], [\"id\", \"feature\"])\n",
        "\n",
        "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
        "\n",
        "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
        "\n",
        "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
        "binarizedDataFrame.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VixXFV-reLX",
        "outputId": "7db35477-138c-4d34-8e83-4f67790d6fa4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binarizer output with Threshold = 0.500000\n",
            "+---+-------+-----------------+\n",
            "| id|feature|binarized_feature|\n",
            "+---+-------+-----------------+\n",
            "|  0|    0.1|              0.0|\n",
            "|  1|    0.8|              1.0|\n",
            "|  2|    0.5|              0.0|\n",
            "|  3|    0.6|              1.0|\n",
            "+---+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.StringIndexer**\n",
        "StringIndexer encodes a string column of labels to a column of label indices. StringIndexer can encode multiple columns. The indices are in [0, numLabels), and four ordering options are supported: frequencyDesc,frequencyAsc,alphabetDesc,alphabetAsc ---Its default value is ‘frequencyDesc’."
      ],
      "metadata": {
        "id": "cohc0HcutQ4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
        "    [\"id\", \"category\"])\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\",stringOrderType=\"frequencyDesc\")\n",
        "indexer.setHandleInvalid(\"skip\")\n",
        "indexed = indexer.fit(df).transform(df)\n",
        "indexed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYmsoEP7tStl",
        "outputId": "e74e170e-2278-4f33-afd2-874c0c4f8968"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-------------+\n",
            "| id|category|categoryIndex|\n",
            "+---+--------+-------------+\n",
            "|  0|       a|          0.0|\n",
            "|  1|       b|          2.0|\n",
            "|  2|       c|          1.0|\n",
            "|  3|       a|          0.0|\n",
            "|  4|       a|          0.0|\n",
            "|  5|       c|          1.0|\n",
            "+---+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.IndexToString**:\n",
        "Symmetrically to StringIndexer, IndexToString maps a column of label indices back to a column containing the original labels as strings. A common use case is to produce indices from labels with StringIndexer, train a model with those indices and retrieve the original labels from the column of predicted indices with IndexToString."
      ],
      "metadata": {
        "id": "madfHyruuGt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IndexToString, StringIndexer\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
        "    [\"id\", \"category\"])\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "model = indexer.fit(df)\n",
        "indexed = model.transform(df)\n",
        "\n",
        "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
        "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
        "indexed.show()\n",
        "\n",
        "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
        "\n",
        "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
        "converted = converter.transform(indexed)\n",
        "\n",
        "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
        "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
        "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCI6X412uJJv",
        "outputId": "9075bdbb-be44-48fb-b896-a8ef3cfb1f73"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed string column 'category' to indexed column 'categoryIndex'\n",
            "+---+--------+-------------+\n",
            "| id|category|categoryIndex|\n",
            "+---+--------+-------------+\n",
            "|  0|       a|          0.0|\n",
            "|  1|       b|          2.0|\n",
            "|  2|       c|          1.0|\n",
            "|  3|       a|          0.0|\n",
            "|  4|       a|          0.0|\n",
            "|  5|       c|          1.0|\n",
            "+---+--------+-------------+\n",
            "\n",
            "StringIndexer will store labels in output column metadata\n",
            "\n",
            "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
            "+---+-------------+----------------+\n",
            "| id|categoryIndex|originalCategory|\n",
            "+---+-------------+----------------+\n",
            "|  0|          0.0|               a|\n",
            "|  1|          2.0|               b|\n",
            "|  2|          1.0|               c|\n",
            "|  3|          0.0|               a|\n",
            "|  4|          0.0|               a|\n",
            "|  5|          1.0|               c|\n",
            "+---+-------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.OneHotEncoder:**\n",
        "One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For string type input data, it is common to encode categorical features using StringIndexer first."
      ],
      "metadata": {
        "id": "VU_iJKTpuXKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (0.0, 1.0),\n",
        "    (1.0, 0.0),\n",
        "    (2.0, 1.0),\n",
        "    (0.0, 2.0),\n",
        "    (0.0, 1.0),\n",
        "    (2.0, 0.0)\n",
        "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
        "\n",
        "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
        "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
        "model = encoder.fit(df)\n",
        "encoded = model.transform(df)\n",
        "encoded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWezrKAubOi",
        "outputId": "0427900f-f79e-4425-a6b6-15d7c234adf9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+-------------+-------------+\n",
            "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
            "+--------------+--------------+-------------+-------------+\n",
            "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
            "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
            "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
            "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
            "+--------------+--------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Bucketizer:**\n",
        "Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter:"
      ],
      "metadata": {
        "id": "k2Tj4TrNu3aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
        "\n",
        "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
        "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
        "dataFrame.show()\n",
        "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
        "\n",
        "# Transform original data into its bucket index.\n",
        "bucketedData = bucketizer.transform(dataFrame)\n",
        "\n",
        "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits()) - 1))\n",
        "bucketedData.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoNgAhPOu2-0",
        "outputId": "3399bb63-a2c3-410b-fbb2-6fa0a246b5ce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -999.9|\n",
            "|    -0.5|\n",
            "|    -0.3|\n",
            "|     0.0|\n",
            "|     0.2|\n",
            "|   999.9|\n",
            "+--------+\n",
            "\n",
            "Bucketizer output with 4 buckets\n",
            "+--------+----------------+\n",
            "|features|bucketedFeatures|\n",
            "+--------+----------------+\n",
            "|  -999.9|             0.0|\n",
            "|    -0.5|             1.0|\n",
            "|    -0.3|             1.0|\n",
            "|     0.0|             2.0|\n",
            "|     0.2|             2.0|\n",
            "|   999.9|             3.0|\n",
            "+--------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.SQLTransformer\":**\n",
        "SQLTransformer implements the transformations which are defined by SQL statement. Currently, we only support SQL syntax like \"SELECT ... FROM __THIS__ ...\" where \"__THIS__\" represents the underlying table of the input dataset."
      ],
      "metadata": {
        "id": "NX7jRJYKvcT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import SQLTransformer\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (0, 1.0, 3.0),\n",
        "    (2, 2.0, 5.0)\n",
        "], [\"id\", \"v1\", \"v2\"])\n",
        "sqlTrans = SQLTransformer(\n",
        "    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__ where v1>0\")\n",
        "sqlTrans.transform(df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VdZPiLivenn",
        "outputId": "c9461ddd-f002-40f0-cd0b-77dada543388"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+---+----+\n",
            "| id| v1| v2| v3|  v4|\n",
            "+---+---+---+---+----+\n",
            "|  0|1.0|3.0|4.0| 3.0|\n",
            "|  2|2.0|5.0|7.0|10.0|\n",
            "+---+---+---+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.VectorAssembler:**\n",
        "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."
      ],
      "metadata": {
        "id": "n3SNryihvtWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "dataset = spark.createDataFrame(\n",
        "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
        "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "output = assembler.transform(dataset)\n",
        "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
        "output.select(\"features\", \"clicked\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O895TN4_vvSK",
        "outputId": "8515e4db-caa6-4e82-88d4-b67289063266"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
            "+-----------------------+-------+\n",
            "|features               |clicked|\n",
            "+-----------------------+-------+\n",
            "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
            "+-----------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.Imputer:**\n",
        "The Imputer estimator completes missing values in a dataset, using the mean, median or mode of the columns in which the missing values are located. The input columns should be of numeric type. Currently Imputer does not support categorical features"
      ],
      "metadata": {
        "id": "RfFwG0SDv_2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1.0, float(\"nan\")),\n",
        "    (2.0, float(\"nan\")),\n",
        "    (float(\"nan\"), 3.0),\n",
        "    (4.0, 4.0),\n",
        "    (5.0, 5.0)\n",
        "], [\"a\", \"b\"])\n",
        "\n",
        "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
        "model = imputer.fit(df)\n",
        "\n",
        "model.transform(df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhgV9_CuwDl4",
        "outputId": "ecd6b0c5-0f20-4b78-94d4-6f7c6edc02be"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-----+-----+\n",
            "|  a|  b|out_a|out_b|\n",
            "+---+---+-----+-----+\n",
            "|1.0|NaN|  1.0|  4.0|\n",
            "|2.0|NaN|  2.0|  4.0|\n",
            "|NaN|3.0|  3.0|  3.0|\n",
            "|4.0|4.0|  4.0|  4.0|\n",
            "|5.0|5.0|  5.0|  5.0|\n",
            "+---+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}